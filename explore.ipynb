{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fd3c8f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T04:33:49.452594Z",
     "iopub.status.busy": "2025-04-08T04:33:49.452280Z",
     "iopub.status.idle": "2025-04-08T04:33:57.123696Z",
     "shell.execute_reply": "2025-04-08T04:33:57.122820Z",
     "shell.execute_reply.started": "2025-04-08T04:33:49.452569Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2854cb56f70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from rich.progress import (\n",
    "    Progress,\n",
    "    TextColumn,\n",
    "    BarColumn,\n",
    "    TimeRemainingColumn,\n",
    "    MofNCompleteColumn,\n",
    ")\n",
    "from transformers import Wav2Vec2Model\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from torchmetrics import MeanSquaredError\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2a948f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T04:33:57.125457Z",
     "iopub.status.busy": "2025-04-08T04:33:57.124866Z",
     "iopub.status.idle": "2025-04-08T04:34:04.048648Z",
     "shell.execute_reply": "2025-04-08T04:34:04.047665Z",
     "shell.execute_reply.started": "2025-04-08T04:33:57.125432Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# secret_value_0 = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "# wandb.login(key = secret_value_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6fd34fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T04:34:04.051263Z",
     "iopub.status.busy": "2025-04-08T04:34:04.050647Z",
     "iopub.status.idle": "2025-04-08T04:34:04.058902Z",
     "shell.execute_reply": "2025-04-08T04:34:04.057845Z",
     "shell.execute_reply.started": "2025-04-08T04:34:04.051231Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AudioGrammarDataset(Dataset):\n",
    "    def __init__(self, data_dir, metadata_path, is_test=False, max_length=1000000):\n",
    "        self.df = pd.read_csv(metadata_path)\n",
    "        self.audio_files = [\n",
    "            os.path.join(data_dir, file) for file in self.df[\"filename\"]\n",
    "        ]\n",
    "        self.is_test = is_test\n",
    "\n",
    "        if not self.is_test:\n",
    "            self.labels = self.df[\"label\"]\n",
    "\n",
    "        self.max_length = (\n",
    "            max_length  # Max length in samples (16kHz * 60 seconds = 960000)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_files[idx]\n",
    "\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "        if sample_rate != 16000:\n",
    "            resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        waveform = waveform.squeeze(0)\n",
    "\n",
    "        if waveform.shape[0] > self.max_length:\n",
    "            waveform = waveform[: self.max_length]\n",
    "            attention_mask = torch.ones(waveform.shape)\n",
    "        else:\n",
    "            padding = torch.zeros(self.max_length - waveform.shape[0])\n",
    "            attention_mask = torch.ones(waveform.shape)\n",
    "            waveform = torch.cat([waveform, padding])\n",
    "            attention_mask = torch.cat([attention_mask, padding])\n",
    "        if not self.is_test:\n",
    "            label = self.labels[idx]\n",
    "\n",
    "            return {\n",
    "                \"raw_waveform\": waveform,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"label\": torch.FloatTensor([label]),\n",
    "            }\n",
    "        else:\n",
    "            return {\"raw_waveform\": waveform, \"attention_mask\": attention_mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea08e61d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T04:34:04.060513Z",
     "iopub.status.busy": "2025-04-08T04:34:04.060165Z",
     "iopub.status.idle": "2025-04-08T04:34:04.077814Z",
     "shell.execute_reply": "2025-04-08T04:34:04.076691Z",
     "shell.execute_reply.started": "2025-04-08T04:34:04.060479Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Wav2Vec2GrammarScoring(nn.Module):\n",
    "    def __init__(self, pretrained_model_name=\"facebook/wav2vec2-base-960h\"):\n",
    "        super(Wav2Vec2GrammarScoring, self).__init__()\n",
    "\n",
    "        self.wav2vec2 = Wav2Vec2Model.from_pretrained(pretrained_model_name)\n",
    "\n",
    "        hidden_size = self.wav2vec2.config.hidden_size\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_values, attention_mask):\n",
    "        outputs = self.wav2vec2(input_values, attention_mask)\n",
    "\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        pooled_output = torch.mean(hidden_states, dim=1)\n",
    "\n",
    "        score = 5 * self.classifier(pooled_output)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def freeze_feature_encoder(self):\n",
    "        for param in self.wav2vec2.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_feature_encoder(self):\n",
    "        for param in self.wav2vec2.feature_extractor.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def freeze_base_model(self):\n",
    "        for param in self.wav2vec2.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_transformer_layers(self, num_layers=4):\n",
    "        self.freeze_base_model()\n",
    "        # Unfreeze the last num_layers transformer layers\n",
    "        for i in range(\n",
    "            len(self.wav2vec2.encoder.layers) - num_layers,\n",
    "            len(self.wav2vec2.encoder.layers),\n",
    "        ):\n",
    "            for param in self.wav2vec2.encoder.layers[i].parameters():\n",
    "                param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb53e8d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T04:34:42.564715Z",
     "iopub.status.busy": "2025-04-08T04:34:42.564348Z",
     "iopub.status.idle": "2025-04-08T04:34:42.579972Z",
     "shell.execute_reply": "2025-04-08T04:34:42.578973Z",
     "shell.execute_reply.started": "2025-04-08T04:34:42.564684Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, config, model = None):\n",
    "        self.config = config\n",
    "        self.device = torch.device(config[\"device\"])\n",
    "        self._prepare_dataloaders()\n",
    "\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "\n",
    "        self.rmse_metric = MeanSquaredError(squared=False).to(self.device)\n",
    "        \n",
    "        if model:\n",
    "            self.model = model.to(device)\n",
    "        else:\n",
    "            self._prepare_model()\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config[\"learning_rate\"],\n",
    "            weight_decay=self.config[\"weight_decay\"],\n",
    "        )\n",
    "\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            self.optimizer, step_size=1, gamma=0.95\n",
    "        )\n",
    "        wandb.init(project=\"wav2vec2-grammar\", config=config)\n",
    "        \n",
    "        self.history = {\n",
    "            \"train_loss\": [],\n",
    "            \"train_rmse\": [],\n",
    "            \"val_loss\": [],\n",
    "            \"val_rmse\": [],\n",
    "        }\n",
    "        \n",
    "        self.best_val_rmse = float(\"inf\")\n",
    "\n",
    "    def _prepare_dataloaders(self):\n",
    "        dataset = AudioGrammarDataset(\n",
    "            data_dir=\"/kaggle/input/shl-intern-hiring-assessment/dataset/audios_train\", \n",
    "            metadata_path=\"/kaggle/input/shl-intern-hiring-assessment/dataset/train.csv\"\n",
    "        )\n",
    "\n",
    "        train_size = int((1 - self.config[\"val_size\"]) * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_data, val_data = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "        self.train_loader = DataLoader(\n",
    "            train_data, batch_size=self.config[\"batch_size\"], shuffle=True\n",
    "        )\n",
    "        self.val_loader = DataLoader(\n",
    "            val_data, batch_size=self.config[\"batch_size\"], shuffle=False\n",
    "        )\n",
    "\n",
    "        print(\"Train and Val dataloaders Prepared !!\")\n",
    "\n",
    "    def _prepare_model(self):\n",
    "        self.model = Wav2Vec2GrammarScoring(\n",
    "            self.config[\"pretrained_model_name\"],\n",
    "        ).to(self.device)\n",
    "        self.model.freeze_feature_encoder()\n",
    "        print(\"Model Initialized !!\")\n",
    "\n",
    "    def train(self):\n",
    "        step = 0\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for epoch in range(self.config[\"num_epochs\"]):\n",
    "            self.model.train()\n",
    "\n",
    "            self.rmse_metric.reset()\n",
    "\n",
    "            with Progress(\n",
    "                TextColumn(\"[bold blue]{task.description}\"),\n",
    "                BarColumn(),\n",
    "                MofNCompleteColumn(),\n",
    "                TimeRemainingColumn(),\n",
    "                TextColumn(\"â€¢ Loss: {task.fields[loss]}\", justify=\"right\"),\n",
    "                transient=True,\n",
    "            ) as progress:\n",
    "                train_task = progress.add_task(\n",
    "                    f\"Epoch {epoch + 1}/{self.config['num_epochs']} [bold white on blue]TRAIN[/]\",\n",
    "                    total=len(self.train_loader),\n",
    "                    loss=\"0.0000\"\n",
    "                )\n",
    "                for batch in self.train_loader:\n",
    "                    waveforms = batch[\"raw_waveform\"].to(self.device)\n",
    "                    attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                    labels = batch[\"label\"].to(self.device).squeeze(1)  # [B, 1]\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "                    outputs = self.model(waveforms, attention_mask)  # Shape: [B, 1]\n",
    "                    loss = self.criterion(outputs.squeeze(1), labels)\n",
    "\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    step += 1\n",
    "                    running_loss += loss.item()\n",
    "                    self.rmse_metric.update(outputs.squeeze(1), labels)\n",
    "\n",
    "                    progress.update(\n",
    "                        train_task,\n",
    "                        advance=1,\n",
    "                        loss=f\"{(running_loss) / (step):.4f}\"\n",
    "                    )\n",
    "                    \n",
    "                    wandb.log({\n",
    "                        \"train_step_loss\": (running_loss) / (step)\n",
    "                    }, step = step)\n",
    "                    \n",
    "                    if step % 100 == 0:\n",
    "                        train_loss = (running_loss) / (step)\n",
    "                        train_rmse = self.rmse_metric.compute().item()\n",
    "            \n",
    "                        val_loss, val_rmse = self.evaluate(epoch, progress)\n",
    "            \n",
    "                        self.history[\"train_loss\"].append(train_loss)\n",
    "                        self.history[\"train_rmse\"].append(train_rmse)\n",
    "                        self.history[\"val_loss\"].append(val_loss)\n",
    "                        self.history[\"val_rmse\"].append(val_rmse)\n",
    "            \n",
    "                        wandb.log(\n",
    "                            {\n",
    "                                \"epoch\": epoch + 1,\n",
    "                                \"train_rmse\": train_rmse,\n",
    "                                \"val_loss\": val_loss,\n",
    "                                \"val_rmse\": val_rmse,\n",
    "                            },\n",
    "                            step=step,\n",
    "                        )\n",
    "            \n",
    "                        if val_rmse < self.best_val_rmse:\n",
    "                            self.best_val_rmse = val_rmse\n",
    "                            model_path = \"best_model.pt\"\n",
    "                            torch.save(self.model.state_dict(), model_path)\n",
    "                            wandb.save(model_path)\n",
    "                            self._log_model_as_artifact(model_path)\n",
    "            \n",
    "                        print(\n",
    "                            f\"[Epoch {epoch + 1}] [Step {step}] Train Loss: {train_loss:.4f}, Train RMSE: {train_rmse:.4f} | Val Loss: {val_loss:.4f}, Val RMSE: {val_rmse:.4f}\"\n",
    "                        )\n",
    "\n",
    "            self.scheduler.step()\n",
    "\n",
    "        wandb.finish()\n",
    "        print(\" Training Completed !!\")\n",
    "\n",
    "    def evaluate(self, epoch, progress):\n",
    "        self.model.eval()\n",
    "        val_loss = 0.0\n",
    "        self.rmse_metric.reset()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_task = progress.add_task(\n",
    "                f\"Epoch {epoch + 1}/{self.config['num_epochs']} [bold white on green]VAL[/]\", \n",
    "                total=len(self.val_loader), \n",
    "                loss=\"0.0000\",\n",
    "            )\n",
    "            for i,batch in enumerate(self.val_loader):\n",
    "                waveforms = batch[\"raw_waveform\"].to(self.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                labels = batch[\"label\"].to(self.device).squeeze(1)\n",
    "    \n",
    "                outputs = self.model(waveforms, attention_mask)\n",
    "                loss = self.criterion(outputs.squeeze(1), labels)\n",
    "                val_loss += loss.item()\n",
    "                self.rmse_metric.update(outputs.squeeze(1), labels)\n",
    "    \n",
    "                progress.update(\n",
    "                    val_task, advance=1, loss=f\"{(val_loss) / (i+1):.4f}\"\n",
    "                )\n",
    "\n",
    "        return val_loss / len(\n",
    "            self.val_loader\n",
    "        ), self.rmse_metric.compute().item()\n",
    "\n",
    "    def _log_model_as_artifact(self, model_path):\n",
    "        artifact = wandb.Artifact(f\"best_model\", type=\"model\")\n",
    "        artifact.add_file(model_path)\n",
    "        wandb.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b15d45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T04:34:51.363891Z",
     "iopub.status.busy": "2025-04-08T04:34:51.363596Z",
     "iopub.status.idle": "2025-04-08T04:34:51.368387Z",
     "shell.execute_reply": "2025-04-08T04:34:51.367353Z",
     "shell.execute_reply.started": "2025-04-08T04:34:51.363869Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"batch_size\": 2,\n",
    "    \"num_epochs\": 8,\n",
    "    \"learning_rate\": 3e-5,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"val_size\": 0.2,\n",
    "    \"random_state\": 42,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"pretrained_model_name\": \"facebook/wav2vec2-base-960h\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c405d65b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T04:34:52.962998Z",
     "iopub.status.busy": "2025-04-08T04:34:52.962701Z",
     "iopub.status.idle": "2025-04-08T04:35:00.072445Z",
     "shell.execute_reply": "2025-04-08T04:35:00.071562Z",
     "shell.execute_reply.started": "2025-04-08T04:34:52.962976Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eadd0f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T04:35:02.170482Z",
     "iopub.status.busy": "2025-04-08T04:35:02.170101Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "history, model = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83425d2f",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Inference:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = torch.device(config[\"device\"] if config[\"device\"] else \"cpu\")\n",
    "        self.scores = []\n",
    "        self.model = Wav2Vec2GrammarScoring(config[\"pretrained_model_name\"]).to(\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        self.model.load_state_dict(\n",
    "            torch.load(config[\"model_path\"], map_location=self.device)\n",
    "        )\n",
    "        self.model.eval()\n",
    "\n",
    "        self.sampling_rate = config.get(\"sampling_rate\", 16000)\n",
    "        print(\"Inference model loaded and ready!\")\n",
    "\n",
    "        self.preprocess()\n",
    "\n",
    "    def preprocess(self):\n",
    "        test_dataset = AudioGrammarDataset(\n",
    "            data_dir=config[\"test_audio_dir\"],\n",
    "            metadata_path=config[\"test_metadata_path\"],\n",
    "            is_test=True,\n",
    "        )\n",
    "\n",
    "        self.test_loader = DataLoader(\n",
    "            test_dataset, batch_size=self.config[\"batch_size\"]\n",
    "        )\n",
    "\n",
    "        print(\"Test_loader initialized !!\")\n",
    "\n",
    "    def predict(self):\n",
    "        \n",
    "        for batch in self.test_loader:\n",
    "            waveform = batch[\"raw_waveform\"].to(self.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = self.model(waveform, attention_mask)\n",
    "                batch_scores = output.squeeze(1).cpu().numpy()\n",
    "\n",
    "                batch_scores = np.clip(batch_scores, 0.0, 5.0)\n",
    "\n",
    "                self.scores.append(batch_scores)\n",
    "\n",
    "        return np.concatenate(self.scores, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d856581b",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"pretrained_model_name\": \"facebook/wav2vec2-base\",\n",
    "    \"sampling_rate\": 16000,\n",
    "    \"model_path\": \"best_model.pt\",\n",
    "    \"test_metadata_path\": \"./dataset/test.csv\",\n",
    "    \"batch_size\": 8,\n",
    "    \"test_audio_dir\": \"./dataset/audios_test\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "395676c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shobh\\miniconda3\\envs\\ML\\Lib\\site-packages\\transformers\\configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference model loaded and ready!\n",
      "Test_loader initialized !!\n"
     ]
    }
   ],
   "source": [
    "inference_engine = Inference(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65148e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = inference_engine.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe519b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a639c737",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 11694977,
     "sourceId": 97919,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
